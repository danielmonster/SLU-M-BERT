{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "32Jj8mygiEws",
    "outputId": "9f1953bb-c58c-4d6f-d5b8-e98a5beb1d34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting allosaurus\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/8c/64fc6fc14e527889d0ee18c2c1b2b109c48ae31e6683de7b60607e65cf17/allosaurus-0.5.2-py3-none-any.whl (51kB)\n",
      "\r",
      "\u001b[K     |██████▍                         | 10kB 14.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▊                   | 20kB 14.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 30kB 9.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 40kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 51kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 61kB 3.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from allosaurus) (0.5.3)\n",
      "Collecting panphon\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/6e/a6928ab77ef8adbc4929c9203471501901d10bdc1d7b2e689fb00b26cf77/panphon-0.19-py2.py3-none-any.whl (72kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 4.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allosaurus) (1.19.5)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from allosaurus) (1.8.1+cu101)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allosaurus) (1.4.1)\n",
      "Requirement already satisfied: resampy in /usr/local/lib/python3.7/dist-packages (from allosaurus) (0.2.2)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from panphon->allosaurus) (2019.12.20)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from panphon->allosaurus) (56.0.0)\n",
      "Collecting munkres\n",
      "  Downloading https://files.pythonhosted.org/packages/90/ab/0301c945a704218bc9435f0e3c88884f6b19ef234d8899fb47ce1ccfd0c9/munkres-1.1.4-py2.py3-none-any.whl\n",
      "Collecting unicodecsv\n",
      "  Downloading https://files.pythonhosted.org/packages/6f/a4/691ab63b17505a26096608cc309960b5a6bdf39e4ba1a793d5f9b1a53270/unicodecsv-0.14.1.tar.gz\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from panphon->allosaurus) (3.13)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->allosaurus) (3.7.4.3)\n",
      "Requirement already satisfied: numba>=0.32 in /usr/local/lib/python3.7/dist-packages (from resampy->allosaurus) (0.51.2)\n",
      "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy->allosaurus) (1.15.0)\n",
      "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.32->resampy->allosaurus) (0.34.0)\n",
      "Building wheels for collected packages: unicodecsv\n",
      "  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for unicodecsv: filename=unicodecsv-0.14.1-cp37-none-any.whl size=10768 sha256=e5fc16a09578537b97947fa4eb20ddb805ba6350163081c011b0cd738a70650d\n",
      "  Stored in directory: /root/.cache/pip/wheels/a6/09/e9/e800279c98a0a8c94543f3de6c8a562f60e51363ed26e71283\n",
      "Successfully built unicodecsv\n",
      "Installing collected packages: munkres, unicodecsv, panphon, allosaurus\n",
      "Successfully installed allosaurus-0.5.2 munkres-1.1.4 panphon-0.19 unicodecsv-0.14.1\n",
      "Collecting tokenizers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3MB 6.7MB/s \n",
      "\u001b[?25hInstalling collected packages: tokenizers\n",
      "Successfully installed tokenizers-0.10.2\n",
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1MB 6.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
      "\u001b[K     |████████████████████████████████| 901kB 40.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Installing collected packages: sacremoses, transformers\n",
      "Successfully installed sacremoses-0.0.45 transformers-4.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install allosaurus\n",
    "!pip install tokenizers\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RARQSfPcjUqs",
    "outputId": "602005ea-0d08-4817-b82a-5e205207e35c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/allosaurus/bin/list_phone.py\", line 15, in <module>\n",
      "    model_path = get_model_path(args.model)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/allosaurus/model.py\", line 27, in get_model_path\n",
      "    resolved_model_name = resolve_model_name(model_name)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/allosaurus/model.py\", line 76, in resolve_model_name\n",
      "    return models[0].name\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "!python -m allosaurus.bin.list_phone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UUy6XgGFmP8T"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "p6V2lbYgj5Zc"
   },
   "outputs": [],
   "source": [
    "# copy from https://github.com/xinjli/allosaurus\n",
    "ipa0 = ['I', 'a', 'aː', 'ã', 'ă', 'b', 'bʲ', 'bʲj', 'bʷ', 'bʼ', 'bː', 'b̞', 'b̤', 'b̥', 'c', 'd', 'dʒ', 'dʲ', 'dː', 'd̚', 'd̥', 'd̪', 'd̯', 'd͡z', 'd͡ʑ', 'd͡ʒ', 'd͡ʒː', 'd͡ʒ̤', 'e', 'eː', 'e̞', 'f', 'fʲ', 'fʷ', 'fː', 'g', 'gʲ', 'gʲj', 'gʷ', 'gː', 'h', 'hʷ', 'i', 'ij', 'iː', 'i̞', 'i̥', 'i̯', 'j', 'k', 'kx', 'kʰ', 'kʲ', 'kʲj', 'kʷ', 'kʷʼ', 'kʼ', 'kː', 'k̟ʲ', 'k̟̚', 'k͡p̚', 'l', 'lʲ', 'lː', 'l̪', 'm', 'mʲ', 'mʲj', 'mʷ', 'mː', 'n', 'nj', 'nʲ', 'nː', 'n̪', 'n̺', 'o', 'oː', 'o̞', 'o̥', 'p', 'pf', 'pʰ', 'pʲ', 'pʲj', 'pʷ', 'pʷʼ', 'pʼ', 'pː', 'p̚', 'q', 'r', 'rː', 's', 'sʲ', 'sʼ', 'sː', 's̪', 't', 'ts', 'tsʰ', 'tɕ', 'tɕʰ', 'tʂ', 'tʂʰ', 'tʃ', 'tʰ', 'tʲ', 'tʷʼ', 'tʼ', 'tː', 't̚', 't̪', 't̪ʰ', 't̪̚', 't͡s', 't͡sʼ', 't͡ɕ', 't͡ɬ', 't͡ʃ', 't͡ʃʲ', 't͡ʃʼ', 't͡ʃː', 'u', 'uə', 'uː', 'u͡w', 'v', 'vʲ', 'vʷ', 'vː', 'v̞', 'v̞ʲ', 'w', 'x', 'x̟ʲ', 'y', 'z', 'zj', 'zʲ', 'z̪', 'ä', 'æ', 'ç', 'çj', 'ð', 'ø', 'ŋ', 'ŋ̟', 'ŋ͡m', 'œ', 'œ̃', 'ɐ', 'ɐ̞', 'ɑ', 'ɑ̱', 'ɒ', 'ɓ', 'ɔ', 'ɔ̃', 'ɕ', 'ɕː', 'ɖ̤', 'ɗ', 'ə', 'ɛ', 'ɛ̃', 'ɟ', 'ɡ', 'ɡʲ', 'ɡ̤', 'ɡ̥', 'ɣ', 'ɣj', 'ɤ', 'ɤɐ̞', 'ɤ̆', 'ɥ', 'ɦ', 'ɨ', 'ɪ', 'ɫ', 'ɯ', 'ɯ̟', 'ɯ̥', 'ɰ', 'ɱ', 'ɲ', 'ɳ', 'ɴ', 'ɵ', 'ɸ', 'ɹ', 'ɹ̩', 'ɻ', 'ɻ̩', 'ɽ', 'ɾ', 'ɾj', 'ɾʲ', 'ɾ̠', 'ʀ', 'ʁ', 'ʁ̝', 'ʂ', 'ʃ', 'ʃʲː', 'ʃ͡ɣ', 'ʈ', 'ʉ̞', 'ʊ', 'ʋ', 'ʋʲ', 'ʌ', 'ʎ', 'ʏ', 'ʐ', 'ʑ', 'ʒ', 'ʒ͡ɣ', 'ʔ', 'ʝ', 'ː', 'β', 'β̞', 'θ', 'χ', 'ә', 'ḁ']\n",
    "ipa1, ipa2, ipa3 = ipa0.copy(), ipa0.copy(), ipa0.copy()\n",
    "random.shuffle(ipa1)\n",
    "random.shuffle(ipa2)\n",
    "random.shuffle(ipa3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ezdVxXpInY7Q"
   },
   "outputs": [],
   "source": [
    "# randomly joined to form training data\n",
    "passage0 = ' '.join(ipa0)\n",
    "passage1 = ' '.join(ipa1)\n",
    "passage2 = ' '.join(ipa2)\n",
    "passage3 = ' '.join(ipa3)\n",
    "data = [passage0, passage1, passage2, passage3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_mKf-G_XnTbH"
   },
   "outputs": [],
   "source": [
    "# setup\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"<unk>\"))\n",
    "# trainer = WordLevelTrainer(vocab_size=300, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "trainer = WordLevelTrainer(vocab_size=300, special_tokens=[\"<s>\",\"<pad>\",\"</s>\",\"<unk>\",\"<mask>\"])\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "s-fcXGmapJW1"
   },
   "outputs": [],
   "source": [
    "# train the tokenizer\n",
    "tokenizer.train_from_iterator(data, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qpLSVuR9pRDP",
    "outputId": "b13bfae5-fdd7-4ef9-9e09-c71a8d8209f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "{'mʲ': 80, 'pʰ': 166, 'ɑ̱': 100, 'gʲj': 128, 'l': 25, 'b̥': 209, 'ɳ': 38, 'lː': 104, 'ɐ': 111, 'ɪ': 141, 'gʷ': 211, 'uə': 178, 'k': 78, 'rː': 219, 'ɨ': 9, 'ɤɐ̞': 84, 'dʒ': 95, 'χ': 15, 'ɣj': 31, 'u': 77, 't̪ʰ': 154, 'ø': 180, 'z': 210, 'ŋ̟': 179, 'd͡ʒ': 189, 'r': 145, 'ʃ͡ɣ': 105, 'vʷ': 85, 'o̞': 90, 'θ': 99, 'kʲj': 6, 's̪': 226, 'tʼ': 197, 'n̪': 46, 'ts': 181, 'n': 83, 'j': 148, 'z̪': 42, 'd͡z': 60, 'çj': 36, 'ɽ': 134, 'ð': 108, 'pː': 10, 'β̞': 7, 'tʰ': 62, 'ɡ̤': 160, 'tɕʰ': 32, 'tsʰ': 135, 'ɯ': 68, 'ә': 142, 'i̥': 35, 'ɔ': 187, 'ɹ': 220, 'ɗ': 115, 't': 96, 'sʼ': 88, 'm': 122, 't͡sʼ': 163, 'd̯': 176, 'ʒ͡ɣ': 153, 'ɵ': 87, 'ɤ': 76, 'hʷ': 102, 't̪': 161, 'i̞': 30, 'ɛ': 114, 'v̞ʲ': 12, 'kx': 159, 'ɒ': 130, 'o̥': 218, 'pf': 69, 'k̟̚': 132, 'd͡ʒ̤': 61, 'ɑ': 208, 'ɛ̃': 217, 'ɾj': 79, 'ɻ̩': 138, 'aː': 151, 'tʃ': 183, 'ɡʲ': 231, 'ɫ': 66, 'd̥': 131, 'tː': 98, 'mʲj': 70, 'ʑ': 200, 'ə': 202, 'p': 191, 'ɴ': 129, 'nʲ': 94, 'ʊ': 177, 'p̚': 17, 'ä': 119, 'nj': 182, 't͡ɕ': 97, 'vː': 144, 'b̤': 158, 'β': 59, 'd͡ʒː': 51, 'u͡w': 18, 'ɹ̩': 74, 'tɕ': 143, 'kʷ': 216, 'b̞': 230, 'iː': 146, 'fʲ': 213, 't͡ʃʼ': 39, 'vʲ': 24, 'a': 55, 'v̞': 121, 'tʂ': 11, 'œ': 124, 'zj': 198, 'ŋ': 206, 'fː': 82, 'bʲ': 174, 'ʋ': 63, 'c': 214, 'œ̃': 215, 'ʃʲː': 229, '<mask>': 4, '<unk>': 3, 'ḁ': 37, 'x̟ʲ': 65, 'ɣ': 48, 'pʲ': 155, 'ç': 186, 'ij': 170, 'ŋ͡m': 26, 'ʌ': 107, 'ɟ': 29, 'uː': 175, 'bː': 14, 'q': 20, '<pad>': 1, 'pʷ': 147, 'ɾ̠': 168, 'oː': 194, 'I': 195, 'h': 204, 'ɔ̃': 93, 'ɥ': 140, 'y': 106, 'bʷ': 137, 'g': 41, 'ɸ': 167, 'i': 199, 'ʒ': 192, 'ɕː': 169, 'sː': 43, 'ɐ̞': 223, 'ɓ': 207, 'd̪': 19, 'ʀ': 188, 'b': 81, 'ɾʲ': 116, 'x': 221, 'ɕ': 162, 'ɤ̆': 222, 'o': 54, 'w': 103, 's': 149, 'ɦ': 50, 't̚': 118, 'pʲj': 227, 'ɡ': 57, 'ɲ': 136, 'ʔ': 73, 'gʲ': 92, 'kʲ': 72, 'mː': 110, 'mʷ': 184, 'bʼ': 196, 'tʲ': 225, 'gː': 157, 'dʲ': 34, 'ɰ': 152, 'ʝ': 171, 'ː': 201, 'ʐ': 5, 'd͡ʑ': 164, 'k͡p̚': 228, 'nː': 8, 'ʉ̞': 190, 'v': 113, 'ʋʲ': 173, 'ʈ': 67, 'ʂ': 117, 'kʼ': 64, '<s>': 0, 'pʷʼ': 126, 'ɖ̤': 139, 'sʲ': 150, 't͡ʃː': 89, 't͡ʃ': 53, 'pʼ': 21, 'e̞': 56, 'ɯ̥': 185, 't͡ʃʲ': 22, 'ă': 13, 't̪̚': 75, 'd̚': 193, 'ɱ': 109, 'ʃ': 156, 'dː': 224, 'l̪': 40, 'tʷʼ': 44, 'ʁ̝': 232, 'k̟ʲ': 28, 'ʏ': 71, 'ã': 127, 't͡s': 58, 'ɻ': 233, 'ɾ': 101, 'ʎ': 45, 'tʂʰ': 27, 'n̺': 112, 'e': 23, 'i̯': 86, 'zʲ': 203, 'kː': 165, 'kʷʼ': 47, 'ʁ': 33, 'kʰ': 49, 'fʷ': 205, 'bʲj': 120, 'eː': 91, 'lʲ': 133, '</s>': 2, 'f': 52, 'ɡ̥': 16, 'ɯ̟': 123, 't͡ɬ': 212, 'd': 125, 'æ': 172}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(passage0).tokens == ipa0)\n",
    "print(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PgNp3bv6p7FZ"
   },
   "outputs": [],
   "source": [
    "tokenizer.save('./ipa_tokenizer.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tonwqPvqqqKd"
   },
   "source": [
    "Load the tokenizer by calling\n",
    "tokenizer = Tokenizer.from_file(path_to_ipa_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IPATokenizer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
